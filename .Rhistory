setwd("A:/Göttingen/Quasi/HW1")
did_data <- read_dta("did_dataset.dta")
View(did_data)
View(did_data)
knitr::opts_chunk$set
#load packages
require(tidyverse)
require(haven)
#set working directory and read data
setwd("A:/Göttingen/Quasi/HW1")
did_data <- read_dta("did_dataset.dta")
View(did_data)
#Simple post treatment comparison between treated and untreated
#subset data to wave 2 survey
wave2 <- filter(did_data, wave==2)
wave2_treat <- filter(wave2, treat==1)
wave2_control <- filter(wave2, treat==0)
mean_post <- mean(wave2_treat$savings_account) - mean(wave2_control$savings_account)
mean_post
require(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
#load packages
library(tidyverse)
library(haven)
#set working directory and read data
setwd("A:/Göttingen/Quasi/HW1")
did_data <- read_dta("did_dataset.dta")
#Check the dataset
View(did_data)
#Simple OLS Regression
reg1 <- lm(savings_account ~ treat, wave2)
summary(reg1)
According to the first assumption, there should be a random selection of population into the treatment group.
date: `r format(Sys.time(), '%d %B %Y')`"
tags: [nothing, nothingness]
exit()
quit()
quit
?pandas
install.packages("pandas")
pip install virtualenv
rm(list=ls())
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(haven)
library(foreign)
library(dplyr)
library(ggplot2)
setwd("A:/Göttingen/Lecture_Development_Economics_I_-_Vorlesung_M.WIWI-VWL.0008/PS4")
getwd()
setwd("A:/Göttingen/Dev_Econ_I/PS4")
setwd("A:/Göttingen/WiSe_2021/Dev_Econ_I/PS4")
mpd <- read_dta("mpd2020.dta")
new_mpd <- mpd[mpd$country == "Belgium" | mpd$country == "Egypt"  | mpd$country == "Iraq",]
new_mpd
ggplot(new_mpd, aes(x = year, y = gdppc, col = country)) + stat_summary(geom = 'line')
ggplot(new_mpd, aes(x = year, y = gdppc, col = country, linetype = 5 )) + stat_summary(geom = 'line') + ggtitle("GDP per capita over time")
ggplot(new_mpd, aes(x = year, y = gdppc, col = country )) + stat_summary(geom = 'line') + ggtitle("GDP per capita over time")
mpd_France <- mpd[mpd$country == "France"]
mpd_France <- mpd[mpd$country == "France",]
mpd_France
mpd_France %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagY, -lagD, -t)
mpd_France %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagD, -t)
mpd_France %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
mpd_France_1280 <- mpd[mpd$country == "France", mpd$year <= "1280",]
mpd_France_1280 <- mpd[mpd$country == "France", mpd$year <= 1280,]
mpd_France_1280 <- mpd[mpd$country == "France", mpd$year =< 1280,]
mpd_France_1280 <- mpd[mpd$country == "France", mpd$year <= 1280,]
mpd_France_1280 <- mpd[mpd$country == "France", mpd$year < 1281,]
mpd_France_1280 <- mpd[mpd$country == "France", mpd$year < "1281",]
mpd_France_1280 <- mpd[mpd$country == "France" | mpd$year < "1281",]
mpd_France_1280
mpd_France_1280 <- mpd[mpd$country == "France",]
mpd_France_1280
mpd_France_1280
#aonther round of subsetting for year until 1280
france_1280 <- mpd_France_1280[mpd_France_1280$year == "1", "1000", "1280"]
#aonther round of subsetting for year until 1280
france_1280 <- mpd_France_1280[mpd_France_1280$year <= "1280"]
#aonther round of subsetting for year until 1280
france_1280 <- mpd_France_1280[mpd_France_1280$year < 1280,]
#aonther round of subsetting for year until 1280
france_1280 <- mpd_France_1280[mpd_France_1280$year < 1281,]
france_1280
france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
#aonther round of subsetting for year until 1280
france_1280 <- mpd_France_1280[1280 <= mpd_France_1280$year <= 2019,]
# Subsetting data limited to only France
France <- mpd[mpd$country == "France",]
#aonther round of subsetting for year until 1280
france_1280 <- France[France$year < 1281,]
france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
#aonther round of subsetting for year until 1280
france_2018 <- France[France$year > 1280 & France$year < 2019]
#aonther round of subsetting for year until 1280
france_2018 <- France[France$year > 1280 & France$year < 2019,]
france_2018
france_2018 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
#calculating the compound annual real GDP per capita growth (1 to 1280)
france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
#calculating the compound annual real GDP per capita growth (1 to 1280)
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
#calculating the compound annual real GDP per capita growth (1 to 1280)
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f
#calculating the compound annual real GDP per capita growth (1 to 1280)
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f
c_growth = mean(cagr_f$CAGR, na.rm = TRUE)
#calculating the compound annual real GDP per capita growth (1 to 1280)
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f
c_growth = mean(cagr_f$CAGR, na.rm = TRUE)
c_growth
#calculating the compound annual real GDP per capita growth (1 to 1280)
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f
c_growth = mean(cagr_f$CAGR, na.rm = TRUE)
c_growth
cagr_f_2 = france_2018 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
f_growth = mean(cagr_f_2$CAGR, na.rm = TRUE)
f_growth
cagr_f_2
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f
cagr_f_2 = france_2018 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
f_growth = mean(cagr_f_2$CAGR, na.omit = TRUE)
f_growth
cagr_f_2 = france_2018 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
f_growth = mean(cagr_f_2$CAGR, na.rm = TRUE)
f_growth
f_growth
cagr_f_2 = france_2018 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f_2
#now the average
f_growth = mean(cagr_f_2$CAGR, na.rm = TRUE)
f_growth
cagr_f_2 = france_2018 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
#now the average
f_growth = mean(cagr_f_2$CAGR, na.rm = TRUE)
f_growth
#calculating the compound annual real GDP per capita growth (1 to 1280)
cagr_f = france_1280 %>%
mutate(lagy = lag(year), lagG = lag(gdppc), t = year - lagy, CAGR = (gdppc/lagG)^(1/t) -1)%>%
select(-lagy, -lagG, -t)
cagr_f
c_growth = mean(cagr_f$CAGR, na.rm = TRUE)
c_growth
library(tinytex)
France
France[France$year == 2018]
France[France$year == 2018,]
f_growth
knit_with_parameters("A:/Göttingen/WiSe_2021/Dev_Econ_I/PS4/PS4_i_tried.Rmd", encoding = "UTF-8")
remotes::install_github("rstudio/reticulate")
library(reticulate)
install_minicond
remotes::install_github("rstudio/reticulate")
library(reticulate)
install_miniconda
remotes::install_github("rstudio/reticulate")
install.packages("remotes")
remotes::install_github("rstudio/reticulate")
library(reticulate)
install_miniconda
install_miniconda()
install.packages("pandas")
getwd()
knitr::opts_chunk$set(echo = TRUE)
setwd("A:/Göttingen/SoSe_2022/Econometrics II/R Sessions")
random_walks <- function(N=25, Rep=1000, verbose=TRUE){
# create an empty vector for the results
results <- rep(0, times=Rep)
# loop: Rep times (from 1 to Rep)
for(i in 1:Rep){
# create a normally distributed random vector x with N entries
x <- cumsum(rnorm(N))
# create a normally distributed random vector y with N entries
y <- cumsum(rnorm(N))
# run regression, and write the 4th entry (p-value) of the 2nd estimator (b2) into vector results
results[i] <- summary(lm(y ~ x))$coefficients[2, 4]
}
print(paste0("1% level: ", sum(results < 0.01) / Rep)) # print share of runs whose p < 0.01
print(paste0("5% level: ", sum(results < 0.05) / Rep))  # print share of runs whose p < 0.05
print(paste0("10% level: ", sum(results < 0.1) / Rep)) # print share of runs whose p < 0.1
if(verbose){
plot(x = 1:N,
y = x,
xlab = "t",
ylim = c(min(x,y), max(x,y)),
ylab = "",
type = "l")
lines(x = 1:N,
y = y,
col = "red")
legend("topleft",
legend = c("x", "y"),
col = c("black", "red"),
pch = "l")
}
return(results)
}
set.seed(1234)
p_25obs <- random_walks()
# T = 100
set.seed(1234)
p_100obs <- random_walks(N=100)
# T = 500
set.seed(1234)
p_500obs <- random_walks(N=500)
rm(list=ls())
rm(list=ls())
install.packages("reticulate")
library(reticulate)
use_python("/usr/local/bin/python")
python
use_virtualenv("myenv")
use_python(python, required = NULL)
getwd()
setwd("C:/Users/Pandey/Data Cleaning")
setwd("C:/Users/Pandey/Desktop/Data Cleaning")
library(tidyverse)
library(scrubr)
install.packages("scrubr")
library(scrubr)
library(scrubr)
rm(list=ls())
steam <- read_csv("steam-200K.csv")
View(steam)
steam <- read.csv("steam-200K.csv")
View(steam)
View(steam)
steam <- read_csv("steam-200K.csv", row.names = F)
steam <- read_csv("steam-200K.csv", row.names = FALSE)
steam <- read_csv("steam-200K.csv", header = FALSE)
steam <- read_csv("steam-200K.csv", header = F)
rm(list=ls())
steam <- read_csv("steam-200K.csv", header = F)
steam <- read_csv(file = "steam-200K.csv", header = F)
steam <- read_csv( "steam-200K.csv", row.names = F)
steam <- read_csv( "steam-200K.csv", row_number = F)
steam <- read_csv( "steam-200K.csv")
steam <- read_csv( "steam-200K.csv", show_col_types = FALSE)
View(steam)
steam <- read_csv( "steam-200K.csv",
show_col_types = FALSE, col_names = F)
View(steam)
names(steam) <- c("used_id", "game_title", "behaviour_name", "value", "x")
View(steam)
steam <- as_data_frame(steam[,1:4])
steam <- as_tibble(steam[,1:4])
View(steam)
ign <- read_csv("ign.csv")
View(ign)
ign <- read_csv("ign.csv",
show_col_types = FALSE)
View(ign)
ign <- as_tibble(ign[,2:11])
View(ign)
ign <- read_csv("ign.csv",
show_col_types = FALSE)
View(ign)
ign_1 <- select(ign, -X1)
ign_1 <- select(ign, -...1)
View(ign_1)
ign <- as_tibble(ign[,2:11])
duplicated_rows <- data_frame(duplicated = duplicated(ing),
row = 1:nrow(ing)) %>%
filter(duplicated == T)
duplicated_rows <- data_frame(duplicated = duplicated(ign),
row = 1:nrow(ing)) %>%
filter(duplicated == T)
duplicated_rows <- data_frame(duplicated = duplicated(ign),
row = 1:nrow(ign)) %>%
filter(duplicated =
duplicated_rows <- data_frame(duplicated = duplicated(ign),
row = 1:nrow(ign)) %>%
filter(duplicated == T)
duplicated_rows <- data_frame(duplicated = duplicated(ign),
row = 1:nrow(ign)) %>%
filter(duplicated == T)
duplicated_rows <- tibble(duplicated = duplicated(ign),
row = 1:nrow(ign)) %>%
filter(duplicated == T)
View(duplicated)
View(duplicated_rows)
ggplot(duplicated_rows, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows")
ggplot(duplicated_rows, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip()
ggplot(duplicated_rows, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip() + scale_x_reverse()
ggplot(duplicated_rows, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip()# + scale_x_reverse()
# plotting duplicated row numbers in black lines
ggplot(duplicated_rows, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip() + scale_x_reverse()
dupli_row <- tibble(duplicated = duplicated(steam),
row = 1:nrow(steam)) %>%
filted(duplicated == T)
dupli_row <- tibble(duplicated = duplicated(steam),
row = 1:nrow(steam)) %>%
filter(duplicated == T)
ggplot(dupli_row, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip() + scale_x_reverse()
head(ign)
head(ign)
head(ign)
duplicated(ign) %>%
sum()
ign_distinct <- distinct(ign)
nrow(ign) - nrow(ign_distinct)
head(steam)
gc()
head(steam)
duplicated(steam) %>%
sum()
duplicated(steam) %>%
sum()
steam_distinct <- distinct(steam)
nrow(steam) - nrow(steam_distinct)
?dedup()
?dedup
install.packages("scrubr")
View(ign_distinct)
ign_distinct_V2 <- ign_distinct %>%
distinct(title, release_day, release_month, release_year)
View(ign_distinct_V2)
row(ign_distinct) - row(ign_distinct_V2)
nrow(ign_distinct) - nrow(ign_distinct_V2)
View(steam)
duplicated(steam)
duplicated(steam)%>%
sum()
dupli_steam <- as_tibble(duplicated(steam))
dupli_row <- tibble(duplicated = duplicated(steam),
row = 1:nrow(steam)) %>%
filter(duplicated == T)
View(dupli_row)
rm(dupli_steam)
View(dupli_row)
steam_unique <- steam %>%
distinct(user_id, game_title, behaviour_name)
steam_unique <- steam %>%
distinct(used_id, game_title, behaviour_name)
steam_unique <- steam %>%
distinct(used_id, game_title, behaviour_name, value )
steam_unique <-steam %>%
distinct(.keep_all = TRUE)
steam_unque <-steam %>%
distinct(.keep_all = TRUE)
rm(steam_unique)
ign_distinct_V2 <- ign_distinct %>%
distinct(.keep_all = TRUE)
nrow(ign_distinct) - nrow(ign_distinct_V2)
library(data.table)
unique(steam, by = used_ide)
unique(steam, by = used_id)
unique(steam, by = used_id, game_title, behaviour_name, value)
unique(steam, by = used_id, game_title, behaviour_name, value)
unique(steam, by = "used_id", "game_title", "behaviour_name", "value")
unique(steam, by = used_id, "game_title", "behaviour_name", value)
unique(steam, by = used_id, str(game_title), str(behaviour_name), value)
steam <- read_csv( "steam-200K.csv",
show_col_types = FALSE, col_names = F)
#add column names
names(steam) <- c("used_id", "game_title",
"behaviour_name", "value", "x")
# drop the last column and converting it to tibble (as_tibble)
steam <- as_tibble(steam[,1:4])
# load ign file
ign <- read_csv("ign.csv",
show_col_types = FALSE)
# drop the column with row numbers
ign <- as_tibble(ign[,2:11])
# same things as above
ign_1 <- select(ign, -...1)
duplicated_rows <- tibble(duplicated = duplicated(ign),
row = 1:nrow(ign)) %>%
filter(duplicated == T)
# plotting duplicated row numbers in black lines
ggplot(duplicated_rows, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip() + scale_x_reverse()
duplicated(ign) %>%
sum()
ign_distinct <- distinct(ign)
nrow(ign) - nrow(ign_distinct
)
View(ign_distinct)
ign_distinct <- unique(ign[,c('title', 'url', 'platform')])
ign_distinct_ <- ign %>%
distinct()
remove(ign_distinct_)
ign_distinct <- ign %>%
distinct()
ign_distinct_title <- ign %>%
distinct(title, .keep_all = TRUE)
ign_distinct_title <- ign_distinct %>%
distinct(title, .keep_all = TRUE)
nrow(ign) - nrow(ign_distinct)
nrow(ign_distinct) - nrow(ign_distinct_title)
ign_distinct_title_platform <- ign_distinct %>%
distrinct(title, platform, .keep_all = T )
ign_distinct_title_platform <- ign_distinct %>%
distinct(title, platform, .keep_all = T )
nrow(ign_distinct - ign_distinct_title_platform)
nrow(ign_distinct) - nrow(ign_distinct_title_platform)
ign_Xbox <- ign_distinct %>%
filter(platform)
ggplot(ign_distinct, aes(platform, release_year)) + geom_boxplot()
ggplot(ign_distinct, aes(platform, release_year)) + geom_boxplot() +
coord_flip()
ggplot(ign_distinct, aes(platform, release_year)) + geom_point() +
coord_flip()
ggplot(ign_distinct, aes(platform, title)) + geom_point() +
coord_flip()
ggplot(ign_distinct, aes(platform, sum(title)) + geom_point() +
coord_flip()
count(ign_distinct$title)
ggplot(ign_distinct, aes(platform, str(title)) + geom_point() +
coord_flip()
ggplot(ign_distinct, aes(platform, score) + geom_point() +
coord_flip()
ggplot(ign_distinct, aes(platform, score)) + geom_point()
ggplot(ign_distinct, aes(platform, score)) + geom_point()
dupli_row <- tibble(duplicated = duplicated(steam),
row = 1:nrow(steam)) %>%
filter(duplicated == T)
## plotting the duplicated row numbers in black lines
ggplot(dupli_row, aes(xintercept = row)) +
geom_vline(aes(xintercept = row)) +
ggtitle("Indexes of duplicated rows") +
coord_flip() + scale_x_reverse()
duplicated(steam) %>%
sum()
steam_distinct <- steam %>%
distinct()
# double check
nrow(steam) - nrow(steam_distinct)
# checked, looks good
View(steam_distinct)
steam_distinct_play <- steam_distinct %>%
filter(behaviour_name = "play")
steam_distinct_play <- steam_distinct %>%
filter(behaviour_name == "play")
steam_distinct_purchase <- steam_distinct %>%
filter(behavious_name == "purchase")
steam_distinct_purchase <- steam_distinct %>%
filter(behaviour_name == "purchase")
nrow(steam_distinct_play) + nrow(steam_distinct_purchase) = nrow(steam_distinct)
nrow(steam_distinct_play) + nrow(steam_distinct_purchase) == nrow(steam_distinct)
steam_distinct_play_filter <- steam_distinct_play %>%
distinct(game_title, user_id, .keep_all = T )
steam_distinct_play_filter <- steam_distinct_play %>%
distinct(game_title, used_id, .keep_all = T )
steam_distinct_play
steam_distinct_rep <- steam_distinct%>%
distinct(game_title, used_id, .keep_all = F )
View(steam_distinct_rep)
steam_distinct_rep <- steam_distinct%>%
distinct(game_title, used_id, .keep_all = TRUE )
View(steam_distinct_rep)
